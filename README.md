# GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation
### [Paper](https://arxiv.org/abs/2509.10454) | [Project Page](https://bagh2178.github.io/GC-VLN/) | [Video](https://cloud.tsinghua.edu.cn/f/f21a13df2bc749bb980a/?dl=1)

> GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation  
> [Hang Yin](https://bagh2178.github.io/)\*, Haoyu Wei\*, [Xiuwei Xu](https://xuxw98.github.io/)$^\dagger$, [Wenxuan Guo](https://gwxuan.github.io/), [Jie Zhou](https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1), [Jiwen Lu](http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/)$^\ddagger$  

\* Equal contribution $\dagger$ Project leader $\ddagger$ Corresponding author


We propose a unified 3D graph representation for <b>zero-shot</b> vision-and-language navigation. By modeling instruction graph as constraints, we can solve the optimal navigation path accordingly. Wrong exploration can also be handled by graph-based backtracking.


## News
- [2025/09/16]: Arxiv and project page available. The code will be released in a few weeks!
- [2025/08/01]: GC-VLN is accepted to CoRL 2025!


## Demo
![demo](./assets/demo.gif)


## Relevant Work
Check out our scene graph-based zero-shot navigation series:
- [SG-Nav](https://bagh2178.github.io/SG-Nav/) for zero-shot object-goal navigation.
- [UniGoal](https://github.com/bagh2178/UniGoal) for zero-shot goal-oriented navigation.


## Citation
```
@article{yin2025gcvln, 
      title={GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation}, 
      author={Hang Yin and Haoyu Wei and Xiuwei Xu and Wenxuan Guo and Jie Zhou and Jiwen Lu},
      journal={arXiv preprint arXiv:2509.10454},
      year={2025}
}
```
